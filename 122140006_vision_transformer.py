# -*- coding: utf-8 -*-
"""122140006_Vision Transformer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CYZPGygT5iOTwvFqJ8pSKia9lbBb0DqJ
"""

!pip install torch torchvision torchaudio timm tqdm scikit-learn matplotlib seaborn pandas

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import timm
from tqdm import tqdm
import time
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import gc

# 1. Konfigurasi Global & Setup Device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_CLASSES = 10
BATCH_SIZE = 16
NUM_EPOCHS = 5
LEARNING_RATE = 1e-5
IMAGE_SIZE = 224

NUM_SAMPLES_TRAIN = 800
NUM_SAMPLES_TEST = 200

print(f"Device: {DEVICE}")
print(f"Image Size: {IMAGE_SIZE}, Batch Size: {BATCH_SIZE}")
print(f"Eksperimen menggunakan total {NUM_SAMPLES_TRAIN + NUM_SAMPLES_TEST} sampel data.")

# 2. DATA LOADING, PREPROCESSING, DAN DATALOADER SETUP

# Define normalization constants for CIFAR-10
NORMALIZE_MEAN = (0.4914, 0.4822, 0.4465)
NORMALIZE_STD = (0.2023, 0.1994, 0.2010)

# Transformasi Data
transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)
])

# Menggunakan CIFAR-10
print("\nLoading CIFAR-10 Dataset (Otomatis Download)...")
original_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
original_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# Proses Pengurangan Dataset (Subset)
indices_train = torch.randperm(len(original_train_dataset))[:NUM_SAMPLES_TRAIN]
train_dataset = Subset(original_train_dataset, indices_train)

indices_test = torch.randperm(len(original_test_dataset))[:NUM_SAMPLES_TEST]
test_dataset = Subset(original_test_dataset, indices_test)

CLASS_NAMES = original_train_dataset.classes

# DataLoaders (dengan pin_memory=True untuk kecepatan GPU)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

print(f"Jumlah Kelas: {NUM_CLASSES}")
print(f"Jumlah data training (Subset): {len(train_dataset)} sampel")
print(f"Jumlah data testing (Subset): {len(test_dataset)} sampel")

# 3. FUNGSI INISIASI MODEL & PENGHITUNGAN PARAMETER

def create_model(model_type, num_classes, pretrained=True):
    """Membuat model ViT atau Swin Transformer dari timm dan menghitung parameter."""

    if model_type == 'ViT':
        # Menggunakan nama standar model base, resolusi diatur via img_size
        model_name = 'vit_base_patch16_224'
    elif model_type == 'Swin':
        # Menggunakan nama standar model base
        model_name = 'swin_base_patch4_window7_224'
    else:
        raise ValueError("Model type not supported.")

    try:
        model = timm.create_model(model_name,
                                 pretrained=pretrained,
                                 num_classes=num_classes,
                                 img_size=IMAGE_SIZE) # Ini yang menyesuaikan ukuran input

        # Pindahkan Model ke Device
        model = model.to(DEVICE)

        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        model_size_mb = (total_params * 4) / (1024 * 1024)

        print(f" Model {model_type} ({model_name})")
        print(f"Total Parameters: {total_params:,}")
        print(f"Trainable Parameters: {trainable_params:,}")
        print(f"Model Size (MB): {model_size_mb:.2f}")

        return model, total_params, model_size_mb

    except Exception as e:
        print(f"Error creating model {model_type}: {e}")
        return None, 0, 0

# 4. FUNGSI VISUALISASI METRIK (LEARNING CURVE & CONFUSION MATRIX)

def plot_metrics(history, conf_matrix, model_name, class_names):
    """Memvisualisasikan Learning Curve dan Confusion Matrix."""

    # Plot Learning Curve (Loss dan Accuracy)
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Training Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title(f'{model_name} Learning Loss Curve (5 Epochs)'); plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Training Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.title(f'{model_name} Learning Accuracy Curve (5 Epochs)'); plt.xlabel('Epochs'); plt.ylabel('Accuracy (%)'); plt.legend()
    plt.show()

    # Plot Confusion Matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label'); plt.ylabel('True Label'); plt.title(f'{model_name} Confusion Matrix')
    plt.show()

# 5. FUNGSI TRAINING DAN EVALUASI

def train_and_evaluate_model(model, model_name, train_loader, test_loader):
    """Melakukan fine-tuning, evaluasi, dan pengukuran metrik."""

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

    print(f"Memulai Fine-Tuning untuk {model_name} ({NUM_EPOCHS} epochs) ")

    # Training Loop
    for epoch in range(NUM_EPOCHS):
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} (Train)"):
            # DATA DIPINDAHKAN KE GPU
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()

        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = 100 * correct_train / total_train
        history['train_loss'].append(epoch_loss); history['train_acc'].append(epoch_acc)

        # Validation Loop
        model.eval()
        val_loss = 0.0; correct_val = 0; total_val = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs.data, 1)
                total_val += labels.size(0); correct_val += (predicted == labels).sum().item()

        val_loss = val_loss / len(test_loader.dataset)
        val_acc = 100 * correct_val / total_val
        history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)

        print(f"[{model_name}] Epoch {epoch+1} - Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {epoch_acc:.2f}%, Val Acc: {val_acc:.2f}%")

        # Kosongkan cache GPU dan memori Python (Optimasi VRAM/RAM)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()


    # Pengukuran Metrik Inferensi Akhir
    print(f" Pengukuran Metrik Akhir dan Inferensi untuk {model_name} ")

    all_preds = []; all_labels = []; total_time = 0; total_samples = 0

    print("Warming up for stable timing...")
    with torch.no_grad():
        for _ in range(10):
            try: model(next(iter(test_loader))[0].to(DEVICE))
            except StopIteration: break

    # Pengukuran Waktu Inferensi
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Mengukur Inferensi"):
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            start_time = time.time()
            outputs = model(inputs)
            end_time = time.time()
            total_time += (end_time - start_time)
            total_samples += inputs.size(0)
            # Prediksi dipindahkan kembali ke CPU untuk perhitungan metrik
            _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Perhitungan Metrik
    avg_inference_time_ms = (total_time / total_samples) * 1000
    throughput = total_samples / total_time
    accuracy = accuracy_score(all_labels, all_preds)

    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(all_labels, all_preds, average=None, zero_division=0)
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)
    conf_matrix = confusion_matrix(all_labels, all_preds)

    # OUTPUT DATA
    print(f"\n[HASIL INFERENSI {model_name}]")
    print(f"Waktu Inferensi Rata-rata per Gambar: {avg_inference_time_ms:.3f} ms")
    print(f"Throughput: {throughput:.2f} gambar/detik")

    print(f"\n[HASIL PERFORMA {model_name}]")
    print(f"Accuracy Keseluruhan: {accuracy*100:.2f}%")
    print(f"Macro Precision: {precision_macro:.4f}, Macro Recall: {recall_macro:.4f}, Macro F1-Score: {f1_macro:.4f}")

    print("\nMetrik Per Kelas :")
    for i, name in enumerate(CLASS_NAMES):
        print(f"   {name}: P={precision_per_class[i]:.4f}, R={recall_per_class[i]:.4f}, F1={f1_per_class[i]:.4f}")

    # Visualisasi
    plot_metrics(history, conf_matrix, model_name, CLASS_NAMES)

    return {
        'accuracy': accuracy,
        'macro_f1': f1_macro,
        'inf_time_ms': avg_inference_time_ms,
        'params_total': None,
        'size_mb': None
    }

# 6. MAIN EXECUTION

if __name__ == "__main__":

    results = {}

    print("\n" + "="*40 + " MEMULAI EKSPERIMEN VIY " + "="*40 + "\n")
    # a. Implementasi dan Training Model ViT
    vit_model, vit_params, vit_size = create_model('ViT', NUM_CLASSES, pretrained=True)
    if vit_model:
        vit_metrics = train_and_evaluate_model(vit_model, 'ViT', train_loader, test_loader)
        vit_metrics['params_total'] = vit_params
        vit_metrics['size_mb'] = vit_size
        results['ViT'] = vit_metrics

    # Bebaskan memori GPU setelah ViT selesai
    del vit_model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()


    print("\n" + "="*40 + " MEMULAI EKSPERIMEN SWIN " + "="*40 + "\n")
    # b. Implementasi dan Training Model Swin Transformer
    swin_model, swin_params, swin_size = create_model('Swin', NUM_CLASSES, pretrained=True)
    if swin_model:
        swin_metrics = train_and_evaluate_model(swin_model, 'Swin', train_loader, test_loader)
        swin_metrics['params_total'] = swin_params
        swin_metrics['size_mb'] = swin_size
        results['Swin'] = swin_metrics

    del swin_model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()


    # --- Ringkasan Akhir untuk Laporan ---
    print("\n" + "="*40 + " RINGKASAN KOMPARATIF AKHIR " + "="*40 + "\n")

    data = []
    if 'ViT' in results:
        data.append({
            'Model': 'ViT',
            'Parameters (Juta)': results['ViT']['params_total'] / 1_000_000,
            'Size (MB)': results['ViT']['size_mb'],
            'Accuracy (%)': results['ViT']['accuracy'] * 100,
            'Macro F1': results['ViT']['macro_f1'],
            'Inf Time/img (ms)': results['ViT']['inf_time_ms']
        })
    if 'Swin' in results:
        data.append({
            'Model': 'Swin',
            'Parameters (Juta)': results['Swin']['params_total'] / 1_000_000,
            'Size (MB)': results['Swin']['size_mb'],
            'Accuracy (%)': results['Swin']['accuracy'] * 100,
            'Macro F1': results['Swin']['macro_f1'],
            'Inf Time/img (ms)': results['Swin']['inf_time_ms']
        })

    df_results = pd.DataFrame(data).set_index('Model')

    print("\n[Tabel Komparatif Hasil Eksperimen]")
    print(df_results.to_markdown(floatfmt=".4f"))

    print("\n\nCatatan Penting untuk Laporan:")
    print(f"Eksperimen menggunakan IMAGE_SIZE={IMAGE_SIZE} dan BATCH_SIZE={BATCH_SIZE} untuk optimasi VRAM.")